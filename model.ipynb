{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPu9CFLHCwmq81rgfPJO9lr"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***This code defines a Deep Q-Network (DQN) using reinforcement learning***\n",
        "***1. Importing Required Libraries***\n",
        "\n",
        "torch: PyTorch library for deep learning.\n",
        "\n",
        "torch.nn: Contains classes for building neural networks.\n",
        "\n",
        "torch.optim: Optimizers for training.\n",
        "\n",
        "torch.nn.functional: Contains activation functions like ReLU.\n",
        "\n",
        "os: Used to manage file paths for saving models."
      ],
      "metadata": {
        "id": "_7nCmDDNQTwY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8eunMA6UPueL",
        "outputId": "bd0002c9-767d-4112-a22e-80a29a90123a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport torch.nn.functional as F\\nimport os\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***2. Defining the Neural Network (Linear_QNet)***\n",
        "\n",
        "input_size: Number of input features (e.g., the game state).\n",
        "\n",
        "hidden_size: Number of neurons in the hidden layer.\n",
        "\n",
        "output_size: Number of possible actions the agent can take.\n",
        "\n",
        "Uses fully connected layers (Linear layers).\n",
        "\n"
      ],
      "metadata": {
        "id": "Y4CM5F8qQxlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "class Linear_QNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size)  # First layer\n",
        "        self.linear2 = nn.Linear(hidden_size, output_size)  # Output layer\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "KnlV9gQNQ9li",
        "outputId": "bb8b2b89-976a-4a6b-d2e9-6942ac63c731"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nclass Linear_QNet(nn.Module):\\n    def __init__(self, input_size, hidden_size, output_size):\\n        super().__init__()\\n        self.linear1 = nn.Linear(input_size, hidden_size)  # First layer\\n        self.linear2 = nn.Linear(hidden_size, output_size)  # Output layer\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***3. Forward Propagation (Computing Q-values)***\n",
        "\n",
        "The first layer is followed by a ReLU activation function.\n",
        "\n",
        "The output layer directly returns the predicted Q-values for each possible action.\n",
        "\n"
      ],
      "metadata": {
        "id": "39-Lp3pfRBcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.linear1(x))  # Apply ReLU activation\n",
        "        x = self.linear2(x)  # Compute final output (Q-values)\n",
        "        return x\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fYtnXLdsRI5f",
        "outputId": "602ab4a6-9df4-4de6-a681-50fa0e9c3574"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'   \\n    def forward(self, x):\\n        x = F.relu(self.linear1(x))  # Apply ReLU activation\\n        x = self.linear2(x)  # Compute final output (Q-values)\\n        return x\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***4. Saving the Model***\n",
        "\n",
        "Saves the model's weights (state_dict()) to a .pth file.\n",
        "\n",
        "Creates a folder if it doesn't exist to store the model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SFuzqKppRL13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " '''\n",
        "    def save(self, file_name='model.pth'):\n",
        "        model_folder_path = './model'\n",
        "        if not os.path.exists(model_folder_path):\n",
        "            os.makedirs(model_folder_path)\n",
        "\n",
        "        file_name = os.path.join(model_folder_path, file_name)\n",
        "        torch.save(self.state_dict(), file_name)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "hmCVA7c-RLbz",
        "outputId": "c8050fb4-65b7-4927-bb9a-4e7ff694790a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"  \\n   def save(self, file_name='model.pth'):\\n       model_folder_path = './model'\\n       if not os.path.exists(model_folder_path):\\n           os.makedirs(model_folder_path)\\n\\n       file_name = os.path.join(model_folder_path, file_name)\\n       torch.save(self.state_dict(), file_name)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***5. Training Class (QTrainer)***\n",
        "\n",
        "lr (Learning Rate): Controls how much the model updates at each step.\n",
        "\n",
        "gamma (Discount Factor): Determines how much future rewards are considered.\n",
        "\n",
        "Adam optimizer: Efficient optimization algorithm.\n",
        "\n",
        "MSELoss: Measures the difference between predicted Q-values and target Q-values.\n",
        "\n"
      ],
      "metadata": {
        "id": "uIOkQfWxRY2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "class QTrainer:\n",
        "    def __init__(self, model, lr, gamma):\n",
        "        self.lr = lr  # Learning rate\n",
        "        self.gamma = gamma  # Discount factor for future rewards\n",
        "        self.model = model\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=self.lr)  # Adam optimizer\n",
        "        self.criterion = nn.MSELoss()  # Mean Squared Error loss function\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "WLlQ61XpRh28",
        "outputId": "b8ec27c8-7372-4c8a-ac51-28e17f2c4756"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nclass QTrainer:\\n    def __init__(self, model, lr, gamma):\\n        self.lr = lr  # Learning rate\\n        self.gamma = gamma  # Discount factor for future rewards\\n        self.model = model\\n        self.optimizer = optim.Adam(model.parameters(), lr=self.lr)  # Adam optimizer\\n        self.criterion = nn.MSELoss()  # Mean Squared Error loss function\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***6. Training Step (train_step)***\n",
        "\n",
        "Converts inputs into PyTorch tensors for training.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZN1oHPH3Rim2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  '''\n",
        "    def train_step(self, state, action, reward, next_state, done):\n",
        "        state = torch.tensor(state, dtype=torch.float)\n",
        "        next_state = torch.tensor(next_state, dtype=torch.float)\n",
        "        action = torch.tensor(action, dtype=torch.long)\n",
        "        reward = torch.tensor(reward, dtype=torch.float)\n",
        "'''"
      ],
      "metadata": {
        "id": "zGOXr31BRssY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Handling Single Data Points***\n",
        "\n",
        "Ensures that a single input is converted into batch format (so it works with PyTorch models).\n",
        "\n"
      ],
      "metadata": {
        "id": "vpWBrEdERvHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "        if len(state.shape) == 1:\n",
        "            state = torch.unsqueeze(state, 0)\n",
        "            next_state = torch.unsqueeze(next_state, 0)\n",
        "            action = torch.unsqueeze(action, 0)\n",
        "            reward = torch.unsqueeze(reward, 0)\n",
        "            done = (done, )\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "B_iYeMlXR1YM",
        "outputId": "5056bc47-e957-47d4-9b70-8644073aa633"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n        if len(state.shape) == 1:\\n            state = torch.unsqueeze(state, 0)\\n            next_state = torch.unsqueeze(next_state, 0)\\n            action = torch.unsqueeze(action, 0)\\n            reward = torch.unsqueeze(reward, 0)\\n            done = (done, )\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***7. Compute Q-values for Current State***\n",
        "\n",
        "The model predicts the Q-values for the current state.\n",
        "\n",
        "The Q-value represents the expected future reward for each action.\n",
        "\n"
      ],
      "metadata": {
        "id": "s-oIRJgGR5gJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "   '''\n",
        "        pred = self.model(state)  # Predict Q-values for current state\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sUv9rMKsR4gR",
        "outputId": "cefd0707-af25-418b-816c-b2197c3ba583"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n     pred = self.model(state)  # Predict Q-values for current state\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***8. Compute Target Q-values***\n",
        "\n",
        "If the game is over (done=True), the Q-value is just the reward.\n",
        "\n",
        "If the game continues (done=False), the Q-value is updated using:\n",
        "\n",
        "ð‘„(ð‘ ,ð‘Ž)=ð‘Ÿ+ð›¾maxð‘„(ð‘ â€²,ð‘Žâ€²)\n",
        "\n",
        "\n",
        "\n",
        "where:\n",
        "\n",
        "ð‘Ÿ is the reward.\n",
        "\n",
        "ð›¾ is the discount factor.\n",
        "\n",
        "maxð‘„(ð‘ â€²,ð‘Žâ€²) is the best future Q-value.\n",
        "\n"
      ],
      "metadata": {
        "id": "_MRH2M0GSJXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    '''\n",
        "        target = pred.clone()  # Copy predictions to modify targets\n",
        "        for idx in range(len(done)):\n",
        "            Q_new = reward[idx]\n",
        "            if not done[idx]:  # If the game is not over, update Q-value\n",
        "                Q_new = reward[idx] + self.gamma * torch.max(self.model(next_state[idx]))\n",
        "\n",
        "            target[idx][torch.argmax(action[idx]).item()] = Q_new\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "IjBDZpdoTC6R",
        "outputId": "aa9d5a66-1f99-4396-bf1f-c52e6d80acd1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n    target = pred.clone()  # Copy predictions to modify targets\\n    for idx in range(len(done)):\\n        Q_new = reward[idx]\\n        if not done[idx]:  # If the game is not over, update Q-value\\n            Q_new = reward[idx] + self.gamma * torch.max(self.model(next_state[idx]))\\n\\n        target[idx][torch.argmax(action[idx]).item()] = Q_new\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***9. Compute Loss and Update Weights***\n",
        "\n",
        "\n",
        "Resets gradients before computing them.\n",
        "\n",
        "Calculates loss using Mean Squared Error between predicted and target Q-values.\n",
        "\n",
        "Performs backpropagation (loss.backward()).\n",
        "\n",
        "Updates weights (step()) using gradient descent.\n",
        "\n"
      ],
      "metadata": {
        "id": "kbeDk-ZCTF3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "        self.optimizer.zero_grad()  # Reset gradients\n",
        "        loss = self.criterion(target, pred)  # Compute loss\n",
        "        loss.backward()  # Compute gradients\n",
        "        self.optimizer.step()  # Update model weights\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hCaqZWzzTPOT",
        "outputId": "93ad4849-445d-4226-d876-cbfc16161e03"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'     \\n        self.optimizer.zero_grad()  # Reset gradients\\n        loss = self.criterion(target, pred)  # Compute loss\\n        loss.backward()  # Compute gradients\\n        self.optimizer.step()  # Update model weights\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "class Linear_QNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.linear1(x))\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "    def save(self, file_name='model.pth'):\n",
        "        model_folder_path = './model'\n",
        "        if not os.path.exists(model_folder_path):\n",
        "            os.makedirs(model_folder_path)\n",
        "\n",
        "        file_name = os.path.join(model_folder_path, file_name)\n",
        "        torch.save(self.state_dict(), file_name)\n",
        "\n",
        "\n",
        "class QTrainer:\n",
        "    def __init__(self, model, lr, gamma):\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.model = model\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=self.lr)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    def train_step(self, state, action, reward, next_state, done):\n",
        "        state = torch.tensor(state, dtype=torch.float)\n",
        "        next_state = torch.tensor(next_state, dtype=torch.float)\n",
        "        action = torch.tensor(action, dtype=torch.long)\n",
        "        reward = torch.tensor(reward, dtype=torch.float)\n",
        "        # (n, x)\n",
        "\n",
        "        if len(state.shape) == 1:\n",
        "            # (1, x)\n",
        "            state = torch.unsqueeze(state, 0)\n",
        "            next_state = torch.unsqueeze(next_state, 0)\n",
        "            action = torch.unsqueeze(action, 0)\n",
        "            reward = torch.unsqueeze(reward, 0)\n",
        "            done = (done, )\n",
        "\n",
        "        # 1: predicted Q values with current state\n",
        "        pred = self.model(state)\n",
        "\n",
        "        target = pred.clone()\n",
        "        for idx in range(len(done)):\n",
        "            Q_new = reward[idx]\n",
        "            if not done[idx]:\n",
        "                Q_new = reward[idx] + self.gamma * torch.max(self.model(next_state[idx]))\n",
        "\n",
        "            target[idx][torch.argmax(action[idx]).item()] = Q_new\n",
        "\n",
        "        # 2: Q_new = r + y * max(next_predicted Q value) -> only do this if not done\n",
        "        # pred.clone()\n",
        "        # preds[argmax(action)] = Q_new\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = self.criterion(target, pred)\n",
        "        loss.backward()\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2bM2ClDkTuRs"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}